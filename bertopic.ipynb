{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting filter words sytem using NLTK libary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\rishi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download(\"words\")\n",
    "english_words_set = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lists(paragraph_list0,label_list0):\n",
    "    paragraph_list1 = [] # list of lists of filtered tokenized words from paragraph1, 2, ...\n",
    "    label_list1 = []\n",
    "\n",
    "    for i in range(len(paragraph_list0)):\n",
    "        word_tokens = nltk.word_tokenize(paragraph_list0[i])\n",
    "        dummy_list = [word for word in word_tokens if word.lower() in english_words_set]\n",
    "        paragraph_list1.append(dummy_list)\n",
    "        word_tokens = nltk.word_tokenize(\" \".join(label_list0[i]))\n",
    "        dummy_list = [word for word in word_tokens if word.lower() in english_words_set]\n",
    "        label_list1.append(dummy_list)\n",
    "    return (paragraph_list1, label_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_list1, label_list1 = zip(*filter_lists(paragraph_list0,label_list0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self,path ='./0.parquet'):\n",
    "        nltk.download(\"words\")\n",
    "        self.english_words_set = set(words.words())\n",
    "        self.df = pd.Dataframe()\n",
    "        self.list0 = []\n",
    "        self.path = path\n",
    "        \n",
    "        \n",
    "    def filter_lists(self):\n",
    "        if(len(self.list0)==0):\n",
    "            self.parquet_traverse()\n",
    "        paragraph_list1 = []\n",
    "        label_list1 = []\n",
    "        for i in range(len(self.paragraph_list0)):\n",
    "            word_tokens = nltk.word_tokenize(self.paragraph_list0[i])\n",
    "            dummy_list = [word for word in word_tokens if word.lower() in self.english_words_set]\n",
    "            paragraph_list1.append(dummy_list)\n",
    "            word_tokens = nltk.word_tokenize(\" \".join(self.label_list0[i]))\n",
    "            dummy_list = [word for word in word_tokens if word.lower() in self.english_words_set]\n",
    "            label_list1.append(dummy_list)\n",
    "        return (paragraph_list1, label_list1)\n",
    "    \n",
    "    def load_parquet(self ):\n",
    "        try:\n",
    "            table = pq.read_table(self.path)\n",
    "            self.df = table.to_pandas()\n",
    "        except Exception as x:\n",
    "            print(x)\n",
    "            return\n",
    "        \n",
    "    def parquet_traverse(self):\n",
    "        if(len(self.df==0)):\n",
    "            self.load_parquet()\n",
    "        for i in range(len(self.df)):\n",
    "            dict1 = self.df.iloc[i,i]\n",
    "            paragraph_list1 = [i for i in dict1.keys()]\n",
    "            label_list1 = [dict1[i] for i in dict1.keys()]\n",
    "            label_list1 = [list(i) for i in label_list1]\n",
    "            self.list0.append([paragraph_list1, label_list1])\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
